{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from collections import Counter\n",
    "from string import digits\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import claims\n",
    "def load_dataset_json(path, instance_num=1e6):\n",
    "    \"\"\"\n",
    "    Reads the Fever Training set, returns list of examples.\n",
    "    instance_num: how many examples to load. Useful for debugging.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "            data.append(json.loads(line))\n",
    "            if iline+1 >= instance_num:\n",
    "                break\n",
    "    return data\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\train.jsonl\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/train.jsonl\"\n",
    "dataset = load_dataset_json(path=path_mac, instance_num=20)\n",
    "\n",
    "# get first 10 verifiable claims\n",
    "# [75397, 150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253].\n",
    "\n",
    "claims_10 = []\n",
    "for i in dataset:\n",
    "    if i['verifiable'] == 'VERIFIABLE':\n",
    "        claims_10.append(i)\n",
    "claims_10 = claims_10[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load docs\n",
    "with open('inverted_word_dictionary.txt', 'rb') as handle:\n",
    "    inverted_word_dictionary = pickle.loads(handle.read())\n",
    "    \n",
    "with open(\"doc_length_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "    doc_length_list = pickle.load(fp)\n",
    "    \n",
    "# load doc\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\wiki_id_text\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/wiki_id_text\"\n",
    "dataset_wikipage = pd.read_table(path_mac,header = None)\n",
    "wikipage = dataset_wikipage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> TF-IDF for claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process claims\n",
    "\n",
    "# numpy\n",
    "# nltk tokenizor\n",
    "#nltk stemmer\n",
    "\n",
    "####### for lemmatize #################\n",
    "def get_pos(a_single_word):\n",
    "    if a_single_word.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif a_single_word.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif a_single_word.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif a_single_word.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def claimProcess(words):\n",
    "    #######  lower case  ############################################################\n",
    "    words = words.lower()\n",
    "    \n",
    "    ####### tokenize ###############################################################\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                          (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "                          |\\$?\\d+(?:,\\d+)*(?:\\.\\d+)?%? # 2,000 or 2.5\n",
    "                          |\\w+(?:[-']\\w+)*      # words w/ optional internal hyphens/apostrophe  e.g. can't\n",
    "                        \"\"\"\n",
    "    word_list = nltk.regexp_tokenize(words, pattern)\n",
    "\n",
    "\n",
    "#     ####### lemmatize ################################################################\n",
    "#     # toooooooooooo slow #############################################################\n",
    "#     word_list = [WordNetLemmatizer().lemmatize(w,pos = get_pos(p) or wordnet.NOUN) for w, p in pos_tag(word_list)] \n",
    "    \n",
    "    ####### remove stop words #######################################################\n",
    "    stopwordlist = set(stopwords.words('english'))\n",
    "    word_list = [w for w in word_list if w not in stopwordlist]\n",
    "    \n",
    "    \n",
    "    ####### lemmatize ##############################################################\n",
    "    #LancasterStemmer().stem('multiply')\n",
    "    #WordNetLemmatizer().lemmatize('birds')\n",
    "    #word_list = [WordNetLemmatizer().lemmatize(SnowballStemmer('english').stem(w),pos='v') for w in word_list]\n",
    "    word_list = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in word_list] # only Lancaster\n",
    "    \n",
    "\n",
    "    return word_list \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCosSimilarity(word_list, inverted_word_dictionary = inverted_word_dictionary, doc_length_list = doc_length_list):  \n",
    "    \"\"\"\n",
    "    word_list is words list of a single claim, the example is shwoed below:\n",
    "    \n",
    "    word_list = claimProcess(claims_10[1]['claim'])\n",
    "    \"\"\"\n",
    "    #build count dic for claim\n",
    "    count_claims = dict(Counter(word_list))\n",
    "\n",
    "    # tf for claim\n",
    "    tf_total_d = len(word_list)\n",
    "    tf_claims = {k:count_claims[k]/tf_total_d for k in count_claims.keys()}\n",
    "    tf_claims_array = np.array(list(tf_claims.values()))\n",
    "    \n",
    "    #tf for docs\n",
    "    tf_doc_list = []\n",
    "    for i in range(len(doc_length_list)):\n",
    "        tf_total_c = doc_length_list[i]\n",
    "        tf_doc = {}\n",
    "        for k in count_claims.keys():\n",
    "            \n",
    "            if tf_total_c:\n",
    "                word_count_in_doc = inverted_word_dictionary[k][\"doc\" + str(i)] if \"doc\" + str(i) in inverted_word_dictionary[k] else 0\n",
    "                tf_doc[k] = word_count_in_doc/tf_total_c\n",
    "            else:\n",
    "                tf_doc[k] = 0\n",
    "                \n",
    "        tf_doc_list.append(tf_doc)\n",
    "    tf_doc_array = np.array([np.array(list(tf_doc_list[i].values())) for i in range(len(tf_doc_list))])\n",
    "\n",
    "    #idf for both claims and docs   \n",
    "    idf = {}\n",
    "    for k in count_claims.keys():\n",
    "        \n",
    "        idf_total = len(doc_length_list)\n",
    "        doc_count_include_k = len(inverted_word_dictionary[k])\n",
    "        idf[k] = math.log(idf_total/(doc_count_include_k+1))\n",
    "        idf_array = np.array(list(idf.values()))\n",
    "\n",
    "    # tf-idf for claims\n",
    "    tf_idf_claims_array = tf_claims_array*idf_array\n",
    "\n",
    "    # tf_idf for docs\n",
    "    tf_idf_docs_array = tf_doc_array*idf_array\n",
    "\n",
    "    # cos similarity\n",
    "    #cos_sim = [dot(tf_idf_claims_array, tf_idf_docs_array[i])/(norm(tf_idf_claims_array)*norm(tf_idf_docs_array[i])) for i in range(len(tf_idf_docs_array))]\n",
    "    cos_sim = []\n",
    "    for i in range(len(tf_idf_docs_array)):\n",
    "        if norm(tf_idf_docs_array[i]) != 0:\n",
    "            cos_sim.append(dot(tf_idf_claims_array, tf_idf_docs_array[i])/(norm(tf_idf_claims_array)*norm(tf_idf_docs_array[i])))\n",
    "        else:\n",
    "            cos_sim.append(0)\n",
    "            \n",
    "    return cos_sim\n",
    "\n",
    "# # use the function for a single claim\n",
    "# word_list = claimProcess(claims_10[5]['claim'])\n",
    "# docs = wikipage\n",
    "# cos_sim = computeCosSimilarity(word_list, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the top 5 doc id for a single claim\n",
    "def getTop5(cos_sim, docs):\n",
    "    #cos_sim.index(max(cos_sim))\n",
    "\n",
    "    top_5_idx = np.argsort(cos_sim)[-5:][::-1]\n",
    "    #top_5_values = [cos_sim[i] for i in top_5_idx]\n",
    "    \n",
    "    #return docs[0][top_5_idx]\n",
    "    return top_5_idx\n",
    "\n",
    "# getTop5(cos_sim, wikipage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the five most similar documents for ten claim and write them to csv\n",
    "# for i in range(0,len(claims_10)):\n",
    "#     docs = wikipage\n",
    "#     word_list = claimProcess(claims_10[i]['claim'])\n",
    "#     cos_sim = computeCosSimilarity(word_list)\n",
    "#     doc_id = getTop5(cos_sim, wikipage)\n",
    "\n",
    "#     with open('q2.csv', \"a\", newline='') as config_csv:\n",
    "#         writer = csv.writer(config_csv)\n",
    "#         writer.writerow([f\"The five most similar documents for Claim {claims_10[i]['id']} \\n\"])\n",
    "#         for top_id in doc_id:\n",
    "#             writer.writerow([str(top_id), str(docs[0][top_id])])\n",
    "#         writer.writerow(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0e1b37955c4d4a91af46a6ad29abc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the five most similar documents for ten claim and write them to csv\n",
    "    \n",
    "with open('q2.csv', \"a\", newline='') as config_csv:\n",
    "    list_save = ['claim id', 'doc id_1', 'doc id_2', 'doc id_3', 'doc id_4', 'doc id_5']\n",
    "    \n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "for i in tqdm(range(0,len(claims_10))):\n",
    "    \n",
    "    word_list = claimProcess(claims_10[i]['claim'])\n",
    "    cos_sim = computeCosSimilarity(word_list)\n",
    "    doc_id = getTop5(cos_sim, wikipage)\n",
    "\n",
    "    with open('q2.csv', \"a\", newline='') as config_csv:\n",
    "        \n",
    "        list_save = [claims_10[i]['id']]\n",
    "        list_save.extend(wikipage[0][doc_id])\n",
    "        \n",
    "        writer = csv.writer(config_csv)\n",
    "        writer.writerow(list_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
