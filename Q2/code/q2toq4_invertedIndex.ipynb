{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000>  Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import random\n",
    "import csv \n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claimProcess(words):\n",
    "    #######  lower case  ############################################################\n",
    "    words = words.lower()\n",
    "    \n",
    "    ####### tokenize ###############################################################\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                          (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "                          |\\$?\\d+(?:,\\d+)*(?:\\.\\d+)?%? # 2,000 or 2.5\n",
    "                          |\\w+(?:[-']\\w+)*      # words w/ optional internal hyphens/apostrophe  e.g. can't\n",
    "                        \"\"\"\n",
    "    word_list = nltk.regexp_tokenize(words, pattern)\n",
    " \n",
    "    ####### remove stop words #######################################################\n",
    "    stopwordlist = set(stopwords.words('english'))\n",
    "    word_list = [w for w in word_list if w not in stopwordlist]\n",
    "    \n",
    "    \n",
    "    ####### stem & lower case ######################################################\n",
    "    word_list = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in word_list] # only Lancaster\n",
    "    \n",
    "\n",
    "    return word_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertedIndex(sentence_list):\n",
    "      \n",
    "    inverted ={}\n",
    "    \n",
    "    for i in tqdm(range(len(sentence_list))):\n",
    "        docname =  \"doc\" + str(i)\n",
    "        single_words_list = claimProcess(sentence_list[i])\n",
    "        \n",
    "        for word in single_words_list:\n",
    "        \n",
    "            if word in inverted:\n",
    "                inverted[word][docname] = single_words_list.count(word)\n",
    "            else:\n",
    "                inverted[word] = {docname: single_words_list.count(word)}\n",
    "                \n",
    "#             indices = inverted.setdefault(word, {})\n",
    "#             indices[docname] = single_words_list.count(word)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\wiki_id_text\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/wiki_id_text\"\n",
    "wikipage = pd.read_table(path_mac,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffc0e4b07c84917930c8331284a966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5395867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inverted_word_dictionary = invertedIndex(wikipage[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "313489\n"
     ]
    }
   ],
   "source": [
    "# get the word count from a doc\n",
    "single_doc_count = inverted_word_dictionary['follow']['doc1']\n",
    "print(single_doc_count)\n",
    "\n",
    "# get the word count from all doc\n",
    "multi_doc_count = np.sum(list(inverted_word_dictionary['follow'].values()))\n",
    "print(multi_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write inverted_word_dictionary\n",
    "import pickle\n",
    "\n",
    "with open('inverted_word_dictionary.txt', 'wb') as handle:\n",
    "    pickle.dump(inverted_word_dictionary, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read inverted_word_dictionary\n",
    "# with open('inverted_word_dictionary.txt', 'rb') as handle:\n",
    "#     inverted_word_dictionary = pickle.loads(handle.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> Save docments length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import token for docs (prepare for tf_docs and idf) \n",
    "with open('q2_doc_words.csv', 'r',encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    doc_words_list = []\n",
    "    for row in reader:\n",
    "        #result[row[0]] = row[1:]\n",
    "        doc_words_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length_list = []\n",
    "for doc in doc_words_list:\n",
    "    doc_length_list.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doc_length_list.txt\", \"wb\") as fp:   \n",
    "    pickle.dump(doc_length_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"doc_length_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     doc_length_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tryyitry = [['i','am','traveling','to','innsbruck','am','traveling'],['i','am','happy']]\n",
    "# #tryyitry = doc_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_count_for_single_inverted(single_words_list):\n",
    "#     inverted_single = {}\n",
    "#     #for i in range(len(words_list)):\n",
    "#     for word in single_words_list:\n",
    "        \n",
    "# #         dest = dict(orig)  # or orig.copy()\n",
    "# #         dest.update(extra)\n",
    "#         inverted_single[word] = single_words_list.count(word)\n",
    "    \n",
    "#     return inverted_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_count_for_multi_inverted(multi_words_list):\n",
    "#     inverted_temp_dict = {}\n",
    "#     for i in range(len(multi_words_list)):\n",
    "#         docname =  \"doc\" + str(i)\n",
    "#         inverted_single = word_count_for_single_inverted(multi_words_list[i])\n",
    "#         inverted_temp_dict[docname] = inverted_single\n",
    "        \n",
    "#     return inverted_temp_dict\n",
    "\n",
    "# inverted_temp_dict = word_count_for_multi_inverted(tryyitry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inverted_index_add(inverted_temp_dict):\n",
    "#     inverted = {}\n",
    "#     for key, values in inverted_temp_dict.items():\n",
    "#         for word, count in values.items():\n",
    "#             indices = inverted.setdefault(word, {})\n",
    "#             indices[key] = count\n",
    "#     return inverted\n",
    "# inverted_word_dictionary = inverted_index_add(inverted_temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inverted_index_add(inverted_temp_dict):\n",
    "#     inverted = {}\n",
    "#     for key, values in inverted_temp_dict.items():\n",
    "#         for word, count in values.items():\n",
    "#             if word in inverted:\n",
    "#                 inverted[word][key] = count\n",
    "#             else:\n",
    "#                 inverted[word] = {key:count}\n",
    "#     return inverted\n",
    "# inverted_word_dictionary = inverted_index_add(inverted_temp_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
