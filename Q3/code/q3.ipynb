{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cengqiqi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from collections import Counter\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load docs\n",
    "with open('inverted_word_dictionary.txt', 'rb') as handle:\n",
    "    inverted_word_dictionary = pickle.loads(handle.read())\n",
    "    \n",
    "with open(\"doc_length_list.txt\", \"rb\") as fp:   # Unpickling\n",
    "    doc_length_list = pickle.load(fp)\n",
    "    \n",
    "# load doc\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\wiki_id_text\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/wiki_id_text\"\n",
    "dataset_wikipage = pd.read_table(path_mac,header = None)\n",
    "wikipage = dataset_wikipage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load claims\n",
    "def load_dataset_json(path, instance_num=1e6):\n",
    "    \"\"\"\n",
    "    Reads the Fever Training set, returns list of examples.\n",
    "    instance_num: how many examples to load. Useful for debugging.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "            data.append(json.loads(line))\n",
    "            if iline+1 >= instance_num:\n",
    "                break\n",
    "    return data\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\train.jsonl\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/train.jsonl\"\n",
    "dataset = load_dataset_json(path=path_mac, instance_num=20)\n",
    "\n",
    "# get first 10 verifiable claims\n",
    "# [75397, 150448, 214861, 156709, 129629, 33078, 6744, 226034, 40190, 76253].\n",
    "\n",
    "claims_10 = []\n",
    "for i in dataset:\n",
    "    if i['verifiable'] == 'VERIFIABLE':\n",
    "        claims_10.append(i)\n",
    "claims_10 = claims_10[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> query-likelihood unigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claimProcess(words):\n",
    "    #######  lower case  ############################################################\n",
    "    words = words.lower()\n",
    "    \n",
    "    ####### tokenize ###############################################################\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                          (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "                          |\\$?\\d+(?:,\\d+)*(?:\\.\\d+)?%? # 2,000 or 2.5\n",
    "                          |\\w+(?:[-']\\w+)*      # words w/ optional internal hyphens/apostrophe  e.g. can't\n",
    "                        \"\"\"\n",
    "    word_list = nltk.regexp_tokenize(words, pattern)\n",
    "    \n",
    "    ####### remove stop words #######################################################\n",
    "    stopwordlist = set(stopwords.words('english'))\n",
    "    word_list = [w for w in word_list if w not in stopwordlist]\n",
    "   \n",
    "    ####### lemmatize ################################################################\n",
    "    word_list = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in word_list] # only Lancaster\n",
    "    \n",
    "\n",
    "    return word_list \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryLikelihood(claim_wordlist):\n",
    "    \n",
    "    p_for_doc_list = []\n",
    "    for i in range(len(doc_length_list)):\n",
    "        if doc_length_list[i]:\n",
    "            p = 1\n",
    "            for word_c in claim_wordlist:\n",
    "                word_count_in_doc = inverted_word_dictionary[word_c][\"doc\" + str(i)] if \"doc\" + str(i) in inverted_word_dictionary[word_c] else 0\n",
    "                p = p*(word_count_in_doc/doc_length_list[i])\n",
    "        else:\n",
    "            p = 0\n",
    "        p_for_doc_list.append(p)\n",
    "        \n",
    "    return p_for_doc_list\n",
    "\n",
    "# claim_wordlist = claimProcess(claims_10[0]['claim'])\n",
    "# p_for_doc_list = queryLikelihood(claim_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the top 5 doc id for a single claim\n",
    "def getTop5(p_for_doc_list):\n",
    "\n",
    "    top_5_idx = np.argsort(p_for_doc_list)[-5:][::-1]\n",
    "    \n",
    "    return top_5_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the five most similar documents for ten claim and write them to csv\n",
    "# for i in range(len(claims_10)):\n",
    "    \n",
    "#     claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "#     p_for_doc_list = queryLikelihood(claim_wordlist)\n",
    "#     doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "#     with open('q3_non_smoothing.csv', \"a\", encoding=\"utf-8\", newline='') as config_csv:\n",
    "#         writer = csv.writer(config_csv)\n",
    "#         writer.writerow([f\"The five most similar documents for Claim {claims_10[i]['id']} \\n\"])\n",
    "#         for top_id in doc_id:\n",
    "#             writer.writerow([str(top_id), str(wikipage[0][top_id])])\n",
    "#         writer.writerow(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457e279186e14c75a7eb65a42a1f6f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the five most similar documents for ten claim and write them to csv\n",
    "    \n",
    "with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "    list_save = ['non_smoothing: ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    list_save = ['claim id', 'doc id_1', 'doc id_2', 'doc id_3', 'doc id_4', 'doc id_5']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "for i in tqdm(range(0,len(claims_10))):\n",
    "    \n",
    "    claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "    p_for_doc_list = queryLikelihood(claim_wordlist)\n",
    "    doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "    with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "        \n",
    "        list_save = [claims_10[i]['id']]\n",
    "        list_save.extend(wikipage[0][doc_id])\n",
    "        \n",
    "        writer = csv.writer(config_csv)\n",
    "        writer.writerow(list_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000>  Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of \n",
    "# a = np.array(doc_words_list)\n",
    "# v = len(np.unique(a))\n",
    "v = len(inverted_word_dictionary)\n",
    "\n",
    "def laplaceSmoothing(claim_wordlist):\n",
    "    \n",
    "    p_for_doc_list = []\n",
    "    for i in range(len(doc_length_list)):\n",
    "        logp = 0\n",
    "        for word_c in claim_wordlist: \n",
    "            word_count_in_doc = inverted_word_dictionary[word_c][\"doc\" + str(i)] if \"doc\" + str(i) in inverted_word_dictionary[word_c] else 0\n",
    "            temp = (1+word_count_in_doc)/(v+doc_length_list[i]) \n",
    "            logp = logp+math.log(temp) # use log to avoid overflow\n",
    "      \n",
    "        p_for_doc_list.append(math.pow(math.e,logp))\n",
    "        \n",
    "                \n",
    "    return p_for_doc_list\n",
    "\n",
    "# claim_wordlist = claimProcess(claims_10[0]['claim'])\n",
    "# p_for_doc_list = laplaceSmoothing(claim_wordlist)\n",
    "# getTop5(p_for_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(claims_10)):\n",
    "    \n",
    "#     claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "#     p_for_doc_list = laplaceSmoothing(claim_wordlist)\n",
    "#     doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "#     with open('q3_laplace_smoothing.csv', \"a\", newline='') as config_csv:\n",
    "#         writer = csv.writer(config_csv)\n",
    "#         writer.writerow([f\"The five most similar documents for Claim {claims_10[i]['id']} \\n\"])\n",
    "#         for top_id in doc_id:\n",
    "#             writer.writerow([str(top_id), str(wikipage[0][top_id])])\n",
    "#         writer.writerow(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f74d7b17e4463691877671feaf54fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the five most similar documents for ten claim and write them to csv\n",
    "    \n",
    "with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "\n",
    "    list_save = [' ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    list_save = ['laplace_smoothing: ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    list_save = ['claim id', 'doc id_1', 'doc id_2', 'doc id_3', 'doc id_4', 'doc id_5'] \n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "for i in tqdm(range(0,len(claims_10))):\n",
    "    \n",
    "    claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "    p_for_doc_list = laplaceSmoothing(claim_wordlist)\n",
    "    doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "    with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "        \n",
    "        list_save = [claims_10[i]['id']]\n",
    "        list_save.extend(wikipage[0][doc_id])\n",
    "        \n",
    "        writer = csv.writer(config_csv)\n",
    "        writer.writerow(list_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000>  Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_words = 0\n",
    "# for i in doc_words_list:\n",
    "#     total_words += len(i)\n",
    "\n",
    "# total_words = 0\n",
    "# for word_c in tqdm(inverted_word_dictionary):\n",
    "#         total_words += np.sum(list(inverted_word_dictionary[word_c].values()))\n",
    "        \n",
    "total_words = 289459793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JMSmoothing(claim_wordlist):\n",
    "    \n",
    "    word_count_in_collection = []\n",
    "    for word_c in claim_wordlist:\n",
    "        word_count_in_collection.append(np.sum(list(inverted_word_dictionary[word_c].values())))\n",
    "        \n",
    "    p_for_doc_list = []\n",
    "    for i in range(len(doc_length_list)):\n",
    "        logp = 0\n",
    "        for j in range(len(claim_wordlist)): \n",
    "            \n",
    "            if doc_length_list[i]:\n",
    "                word_count_in_doc = inverted_word_dictionary[claim_wordlist[j]][\"doc\" + str(i)] if \"doc\" + str(i) in inverted_word_dictionary[claim_wordlist[j]] else 0\n",
    "                temp = 0.5*(word_count_in_doc/doc_length_list[i]) + 0.5*(word_count_in_collection[j]/total_words)\n",
    "            else:\n",
    "                temp = 0.5*(word_count_in_collection[j]/total_words)\n",
    "                \n",
    "            logp = logp+math.log(temp)\n",
    "      \n",
    "        p_for_doc_list.append(math.pow(math.e,logp))\n",
    "        \n",
    "                \n",
    "    return p_for_doc_list\n",
    "\n",
    "# claim_wordlist = claimProcess(claims_10[1]['claim'])\n",
    "# p_for_doc_list = laplaceSmoothing(claim_wordlist)\n",
    "# getTop5(p_for_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(claims_10)):\n",
    "    \n",
    "#     claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "#     p_for_doc_list = JMSmoothing(claim_wordlist)\n",
    "#     doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "#     with open('q3_JM_smoothing.csv', \"a\", newline='') as config_csv:\n",
    "#         writer = csv.writer(config_csv)\n",
    "#         writer.writerow([f\"The five most similar documents for Claim {claims_10[i]['id']} \\n\"])\n",
    "#         for top_id in doc_id:\n",
    "#             writer.writerow([str(top_id), str(wikipage[0][top_id])])\n",
    "#         writer.writerow(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3e7ebf7b56466088ef6a3cdf5b11dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the five most similar documents for ten claim and write them to csv\n",
    "    \n",
    "with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "    \n",
    "    list_save = [' ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    list_save = ['JM_smoothing: ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    list_save = ['claim id', 'doc id_1', 'doc id_2', 'doc id_3', 'doc id_4', 'doc id_5'] \n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "for i in tqdm(range(0,len(claims_10))):  \n",
    "    \n",
    "    claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "    p_for_doc_list = JMSmoothing(claim_wordlist)\n",
    "    doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "    with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "        \n",
    "        list_save = [claims_10[i]['id']]\n",
    "        list_save.extend(wikipage[0][doc_id])\n",
    "        \n",
    "        writer = csv.writer(config_csv)\n",
    "        writer.writerow(list_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000>  Dirichlet Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 289459793\n",
    "#ave_length = math.ceil(total_words/len(doc_length_list))\n",
    "ave_length = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DirichletSmoothing(claim_wordlist):\n",
    "    \n",
    "    word_count_in_collection = []\n",
    "    for word_c in claim_wordlist:\n",
    "        word_count_in_collection.append(np.sum(list(inverted_word_dictionary[word_c].values())))\n",
    "           \n",
    "    p_for_doc_list = []\n",
    "    for i in range(len(doc_length_list)):\n",
    "        logp = 0\n",
    "        for j in range(len(claim_wordlist)): \n",
    "            if doc_length_list[i]:\n",
    "                word_count_in_doc = inverted_word_dictionary[claim_wordlist[j]][\"doc\" + str(i)] if \"doc\" + str(i) in inverted_word_dictionary[claim_wordlist[j]] else 0\n",
    "                temp1 = (doc_length_list[i]/(doc_length_list[i]+ave_length))*(word_count_in_doc/doc_length_list[i])\n",
    "            else:\n",
    "                temp1 = 0\n",
    "            temp2 = (ave_length/(doc_length_list[i]+ave_length))*(word_count_in_collection[j]/total_words)\n",
    "            temp = temp1 + temp2\n",
    "            logp = logp+math.log(temp)\n",
    "      \n",
    "        p_for_doc_list.append(math.pow(math.e,logp))\n",
    "        \n",
    "                \n",
    "    return p_for_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(claims_10)):\n",
    "    \n",
    "#     claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "#     p_for_doc_list = DirichletSmoothing(claim_wordlist)\n",
    "#     doc_id = getTop5(p_for_doc_list)\n",
    "\n",
    "#     with open('q3_Dirichlet_smoothing.csv', \"a\", newline='') as config_csv:\n",
    "#         writer = csv.writer(config_csv)\n",
    "#         writer.writerow([f\"The five most similar documents for Claim {claims_10[i]['id']} \\n\"])\n",
    "#         for top_id in doc_id:\n",
    "#             writer.writerow([str(top_id), str(wikipage[0][top_id])])\n",
    "#         writer.writerow(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12aabc7a77849e0878fd80b3d6a03d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the five most similar documents for ten claim and write them to csv\n",
    "    \n",
    "with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "    \n",
    "    list_save = [' ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "        \n",
    "    list_save = ['Dirichlet_smoothing: ']\n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "    \n",
    "    list_save = ['claim id', 'doc id_1', 'doc id_2', 'doc id_3', 'doc id_4', 'doc id_5']   \n",
    "    writer = csv.writer(config_csv)\n",
    "    writer.writerow(list_save)\n",
    "    \n",
    "for i in tqdm(range(0,len(claims_10))):  \n",
    "    \n",
    "    claim_wordlist = claimProcess(claims_10[i]['claim'])\n",
    "    p_for_doc_list = DirichletSmoothing(claim_wordlist)\n",
    "    doc_id = getTop5(p_for_doc_list)\n",
    "    \n",
    "    with open('q3.csv', \"a\", newline='') as config_csv:\n",
    "        \n",
    "        list_save = [claims_10[i]['id']]\n",
    "        list_save.extend(wikipage[0][doc_id])\n",
    "        \n",
    "        writer = csv.writer(config_csv)\n",
    "        writer.writerow(list_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
