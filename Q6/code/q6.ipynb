{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amOjckqx4Z3i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from string import digits\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "sseo8RtcfVd7",
    "outputId": "1f0a5c35-075d-47e4-c9a7-20e5d28f8cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Package 'python-software-properties' has no installation candidate\n",
      "Selecting previously unselected package google-drive-ocamlfuse.\n",
      "(Reading database ... 131304 files and directories currently installed.)\n",
      "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
      "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "··········\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "Please enter the verification code: Access token retrieved correctly.\n"
     ]
    }
   ],
   "source": [
    "  !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "  !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "  !apt-get update -qq 2>&1 > /dev/null\n",
    "  !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  creds = GoogleCredentials.get_application_default()\n",
    "  import getpass\n",
    "  !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "  vcode = getpass.getpass()\n",
    "  !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "  !mkdir -p drive\n",
    "  !google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aZi6bDN25MmM",
    "outputId": "d1311d80-5490-4bec-b7cf-f46e04bbcc16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki-pages',\n",
       " 'wikipages',\n",
       " 'DM.ipynb',\n",
       " 'q6_input_y_dev',\n",
       " 'q6_input_x_dev',\n",
       " 'q6_input_x_train',\n",
       " 'q6_input_y_train',\n",
       " 'q6_vocab',\n",
       " 'q6_glove_model',\n",
       " 'q6_embedding_matrix',\n",
       " 'q6.ipynb']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/content/drive/colab notebooks/DM\"\n",
    "os.chdir(path)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKhB6oXG4Z3r"
   },
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZYlZdi94Z3t"
   },
   "outputs": [],
   "source": [
    "#load claims\n",
    "def load_dataset_json(path, instance_num=1e6):\n",
    "    \"\"\"\n",
    "    Reads the Fever Training set, returns list of examples.\n",
    "    instance_num: how many examples to load. Useful for debugging.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as openfile:\n",
    "        for iline, line in enumerate(openfile.readlines()):\n",
    "            data.append(json.loads(line))\n",
    "            if iline+1 >= instance_num:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "###train\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\train.jsonl\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/train.jsonl\"\n",
    "dataset = load_dataset_json(path=path_mac)\n",
    "\n",
    "claims = []\n",
    "for i in dataset:\n",
    "    if i['verifiable'] == 'VERIFIABLE':\n",
    "        claims.append(i)\n",
    "\n",
    "        \n",
    "###dev\n",
    "path_windows = \"N:\\\\DesktopSettings\\\\Desktop\\\\DM_working\\\\dataset\\\\dev.jsonl\"\n",
    "path_mac = \"/Users/cengqiqi/Desktop/DM_working/dataset/dev.jsonl\"\n",
    "dataset = load_dataset_json(path=path_mac)\n",
    "\n",
    "claims_dev = []\n",
    "for i in dataset:\n",
    "    if i['verifiable'] == 'VERIFIABLE':\n",
    "        claims_dev.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EraPB6Is4Z3z",
    "outputId": "1f1f041e-7486-4155-830a-0ed0492fcdce",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eb0603f5714bad8fcee72ad91f0824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=109), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load docs lines ()\n",
    "# format: doc_lines['Alphabetical_list_of_comunes_of_Italy'][1]\n",
    "\n",
    "def read_lines(wikipedia_dir):\n",
    "\n",
    "    # doc_id_text saves the title and content of each wiki-page\n",
    "    doc_id_text=dict()\n",
    "\n",
    "    for i in tqdm(range(1,110)):# jsonl file number from 001 to 109\n",
    "        jnum=\"{:03d}\".format(i)\n",
    "        fname=wikipedia_dir+\"wiki-\"+jnum+\".jsonl\"\n",
    "        with open(fname) as f:\n",
    "            # point=f.tell()# file pointer starting from 0\n",
    "            line=f.readline()\n",
    "            while line:\n",
    "                data=json.loads(line.rstrip(\"\\n\"))\n",
    "                doc_id=data[\"id\"] \n",
    "                text = data[\"text\"]\n",
    "                lines=data[\"lines\"]\n",
    "                if text != \"\":\n",
    "                    doclines = {}\n",
    "                    for l in lines.split(\"\\n\"):\n",
    "                        fields = l.split(\"\\t\")\n",
    "                        if fields[0].isnumeric():\n",
    "                            l_id = int(fields[0])\n",
    "                            l_txt = fields[1]\n",
    "                            doclines[l_id] = l_txt\n",
    "\n",
    "                    doc_id_text[doc_id]=doclines\n",
    "                # point=f.tell()\n",
    "                line=f.readline()\n",
    "\n",
    "    return doc_id_text\n",
    "\n",
    "wikipedia_dir = '/Users/cengqiqi/Desktop/DM_working/dataset/wiki-pages/wiki-pages/'\n",
    "\n",
    "doc_lines = read_lines(wikipedia_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IEBdnR74Z35",
    "outputId": "ecd10f3e-d9f5-4285-eedf-2c55863771c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d7f63675034d3fa06b81ee015b2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=109810), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ab5a3d40d646c6a1f6c31c6b5c2727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13332), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def getXandY(claims):\n",
    "    \n",
    "    claim_list = []\n",
    "    sentence_list = []\n",
    "    y_list = []\n",
    "    for i in tqdm(range(len(claims))):\n",
    "        for cla_1 in claims[i]['evidence']:\n",
    "            for cla_2 in cla_1:\n",
    "                title = str(cla_2[2])\n",
    "                sentence = cla_2[3]\n",
    "\n",
    "                try:\n",
    "                    sentence_list.append(doc_lines[title][sentence])    \n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    claim_list.append(claims[i]['claim'])\n",
    "                    if claims[i]['label'] == 'SUPPORTS':\n",
    "                        y_list.append(1)\n",
    "                    else:\n",
    "                        y_list.append(0)\n",
    "    return [claim_list, sentence_list, y_list]\n",
    "\n",
    "claim_list, sentence_list, y_list = getXandY(claims)\n",
    "claim_dev_list, sentence_dev_list, y_dev_list= getXandY(claims_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86IQaUtz4Z4D"
   },
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> 2. Prepare embedding models and Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4YcKgVw4Z4F"
   },
   "outputs": [],
   "source": [
    "# import glove model \n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "glove_model = loadGloveModel(\"/Users/cengqiqi/Desktop/DM_working/glove/glove.6B.300d.txt\")\n",
    "\n",
    "\n",
    "### this glove_model will be updated in sumPooling() method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXZJYhoF4Z4K"
   },
   "outputs": [],
   "source": [
    "# process the sentence (to a word list before embedding)\n",
    "def sentenceProcess(sentence):\n",
    "    #######  lower case  ############################################################\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    ####### tokenize ###############################################################\n",
    "    pattern = r\"\"\"(?x)                  \n",
    "                          (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A. \n",
    "                          |\\$?\\d+(?:,\\d+)*(?:\\.\\d+)?%? # 2,000 or 2.5\n",
    "                          |\\w+(?:[-']\\w+)*      # words w/ optional internal hyphens/apostrophe  e.g. can't\n",
    "                        \"\"\"\n",
    "    word_list = nltk.regexp_tokenize(sentence, pattern)\n",
    "    \n",
    "    \n",
    "    ####### remove rrb #############################################################\n",
    "#     for word in word_list:\n",
    "#         if word in ['rrb', 'lrb','lsb','rsb']:\n",
    "#             word_list.remove(word)\n",
    "    \n",
    "    ### do not remove stopwords in this case\n",
    "    ####### remove stop words #######################################################\n",
    "#     stopwordlist = set(stopwords.words('english'))\n",
    "#     word_list = [w for w in word_list if w not in stopwordlist]\n",
    "   \n",
    "    ####### lemmatize ################################################################\n",
    "    word_list = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in word_list] # only Lancaster\n",
    "    \n",
    "    return word_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgjJ9udi4Z4N",
    "outputId": "bb0666bb-b038-445c-90ac-6d4a68129104",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7568caaf084850a893b3972a897172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=261975), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train glove model and build vocab\n",
    "\n",
    "def trainGloveAndVocab():\n",
    "    temp_word_list = []\n",
    "\n",
    "    for i in tqdm(range(len(claim_list))):\n",
    "        for word in sentenceProcess(claim_list[i]):\n",
    "            if word not in glove_model:\n",
    "                # if word not in glove model, update glove model\n",
    "                embedding_model = Word2Vec([[word]], size=300, window=5, min_count=1, workers=2)\n",
    "                additional_word = {w: vec for w, vec in zip(embedding_model.wv.index2word, embedding_model.wv.vectors)}\n",
    "                glove_model.update(additional_word)\n",
    "            temp_word_list.append(word)\n",
    "\n",
    "        for word in sentenceProcess(sentence_list[i]):\n",
    "            if word not in glove_model:\n",
    "                # if word not in glove model, update glove model\n",
    "                embedding_model = Word2Vec([[word]], size=300, window=5, min_count=1, workers=2)\n",
    "                additional_word = {w: vec for w, vec in zip(embedding_model.wv.index2word, embedding_model.wv.vectors)}\n",
    "                glove_model.update(additional_word)\n",
    "            temp_word_list.append(word)\n",
    "\n",
    "    # vectorizer = CountVectorizer()\n",
    "    # X = vectorizer.fit_transform(temp_word_list)\n",
    "    # vocab=vectorizer.get_feature_names()\n",
    "\n",
    "    vocab = list(np.unique(temp_word_list))\n",
    "    \n",
    "    return [glove_model, vocab]\n",
    "\n",
    "glove_model, vocab = trainGloveAndVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xXMqMTM4Z4T"
   },
   "outputs": [],
   "source": [
    "with open('q6_glove_model', 'wb') as f:\n",
    "    pickle.dump(glove_model, f)\n",
    "with open('q6_vocab', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meWxkAVm4Z4d"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab) + 1, 300))\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"q6_embedding_matrix\", \"wb\") as fp: \n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54o-2RbT4Z4g",
    "outputId": "cb0d5161-7871-42c8-ffc4-9c7969f122ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3df74cbfdd046689e3314098628885e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28341), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getInputX(claim_list, sentence_list):\n",
    "\n",
    "    input_x1_train = []\n",
    "    input_x2_train = []\n",
    "\n",
    "    for i in tqdm(range(len(claim_list))):\n",
    "        c = []\n",
    "        for word in sentenceProcess(claim_list[i]):\n",
    "            try:\n",
    "                c.append(vocab.index(word))\n",
    "            except ValueError:\n",
    "                pass\n",
    "#                 vocab.append(word)\n",
    "#                 c.append(len(vocab)-1)\n",
    "        input_x1_train.append(c)   \n",
    "\n",
    "        s = []\n",
    "        for word in sentenceProcess(sentence_list[i]):\n",
    "            try:\n",
    "                s.append(vocab.index(word))\n",
    "            except ValueError:\n",
    "                pass\n",
    "#                 vocab.append(word)\n",
    "#                 s.append(len(vocab)-1)\n",
    "        input_x2_train.append(s)   \n",
    "\n",
    "    return [input_x1_train, input_x2_train]\n",
    "\n",
    "input_x1_train, input_x2_train = getInputX(claim_list, sentence_list)\n",
    "input_x1_dev, input_x2_dev =  getInputX(claim_dev_list, sentence_dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIejL7Vz4Z4j",
    "outputId": "07852531-aaee-4a1e-ac9d-c5b33ff575cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261975\n",
      "261975\n",
      "261975\n",
      "28341\n",
      "28341\n",
      "28341\n"
     ]
    }
   ],
   "source": [
    "print(len(input_x1_train))\n",
    "print(len(input_x2_train))\n",
    "print(len(y_list))\n",
    "print(len(input_x1_dev))\n",
    "print(len(input_x2_dev))\n",
    "print(len(y_dev_list))\n",
    "\n",
    "#save the results\n",
    "# with open('q6_input_x1_train_no_padding', 'wb') as f:\n",
    "#     pickle.dump(input_x1_train, f)\n",
    "\n",
    "# with open('q6_input_x2_train_no_padding', 'wb') as f:\n",
    "#     pickle.dump(input_x2_train, f)\n",
    "    \n",
    "# with open('q6_input_x1_dev_no_padding', 'wb') as f:\n",
    "#     pickle.dump(input_x1_dev, f)\n",
    "    \n",
    "# with open('q6_input_x2_dev_no_padding', 'wb') as f:\n",
    "#     pickle.dump(input_x2_dev, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KC_yhfp84Z4m"
   },
   "outputs": [],
   "source": [
    "# with open(\"q6_input_x1_train_no_padding\", \"rb\") as fp:   # Unpickling\n",
    "#     input_x1_train = pickle.load(fp)\n",
    "    \n",
    "# with open(\"q6_input_x2_train_no_padding\", \"rb\") as fp:   # Unpickling\n",
    "#     input_x2_train = pickle.load(fp)\n",
    "    \n",
    "# with open(\"q6_input_x1_dev_no_padding\", \"rb\") as fp:   # Unpickling\n",
    "#     input_x1_dev = pickle.load(fp)\n",
    "    \n",
    "# with open(\"q6_input_x2_dev_no_padding\", \"rb\") as fp:   # Unpickling\n",
    "#     input_x2_dev = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2b6MEcF4Z4q"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# padding\n",
    "input_x1_train = pad_sequences(input_x1_train,padding='post')\n",
    "input_x2_train = pad_sequences(input_x2_train,padding='post')\n",
    "\n",
    "input_x1_dev = pad_sequences(input_x1_dev,maxlen = input_x1_train.shape[1],padding='post')\n",
    "input_x2_dev = pad_sequences(input_x2_dev, maxlen = input_x2_train.shape[1],padding='post')\n",
    "\n",
    "input_x_train = np.concatenate((input_x1_train, input_x2_train), axis=1)\n",
    "input_x_dev = np.concatenate((input_x1_dev, input_x2_dev), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mY4rwCoV4Z4s"
   },
   "outputs": [],
   "source": [
    "input_y_train = np.asarray(y_list)\n",
    "input_y_dev = np.asarray(y_dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nakaRAns4Z4u"
   },
   "outputs": [],
   "source": [
    "with open('q6_input_x_train', 'wb') as f:\n",
    "    pickle.dump(input_x_train, f)\n",
    "    \n",
    "with open('q6_input_x_dev', 'wb') as f:\n",
    "    pickle.dump(input_x_dev, f)\n",
    "\n",
    "with open('q6_input_y_train', 'wb') as f:\n",
    "    pickle.dump(input_y_train, f)\n",
    "    \n",
    "with open('q6_input_y_dev', 'wb') as f:\n",
    "    pickle.dump(input_y_dev, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCDwltIF4Z4x"
   },
   "source": [
    "<font face=\"微软雅黑\" size=6 color=#000000> 3. Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "JGe_frTbSHUB",
    "outputId": "00afcd14-ea03-473e-afbf-c26fdd5c4bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-metrics\n",
      "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras-metrics) (2.2.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.2.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras-metrics) (1.16.3)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "egg60SNc4Z4y",
    "outputId": "034417a2-c38b-4e17-9cf2-e38aaeff9fe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import nerual network pakage\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input, Lambda, Dense,Flatten, LSTM, Bidirectional,Activation,Dropout, Conv1D, MaxPooling1D,Flatten, regularizers\n",
    "from keras.models import Model\n",
    "import keras_metrics\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "O9fU_t6U4Z4z",
    "outputId": "aae1e6bd-2894-4ac6-9289-a1cd2563406b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261975, 224)\n",
      "(28341, 224)\n",
      "(261975, 1)\n",
      "(28341, 1)\n"
     ]
    }
   ],
   "source": [
    "# load input\n",
    "with open(\"q6_input_x_train\", \"rb\") as fp:\n",
    "    input_x_train = pickle.load(fp)\n",
    "with open(\"q6_input_x_dev\", \"rb\") as fp:  \n",
    "    input_x_dev = pickle.load(fp)\n",
    "with open(\"q6_input_y_train\", \"rb\") as fp:  \n",
    "    input_y_train = pickle.load(fp)\n",
    "with open(\"q6_input_y_dev\", \"rb\") as fp:   \n",
    "    input_y_dev = pickle.load(fp)\n",
    "\n",
    "input_y_train = np.reshape(input_y_train, (-1,1))\n",
    "input_y_dev = np.reshape(input_y_dev, (-1,1))\n",
    "\n",
    "print(input_x_train.shape)\n",
    "print(input_x_dev.shape)\n",
    "print(input_y_train.shape)\n",
    "print(input_y_dev.shape)\n",
    "\n",
    "\n",
    "with open(\"q6_glove_model\", \"rb\") as fp:\n",
    "    glove_model = pickle.load(fp)    \n",
    "with open(\"q6_vocab\", \"rb\") as fp:   \n",
    "    vocab = pickle.load(fp)\n",
    "with open(\"q6_embedding_matrix\", \"rb\") as fp:   \n",
    "    embedding_matrix = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "09pF3xHy4Z42",
    "outputId": "ff49f3d8-8052-44cb-d130-3165bbaa7ec8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(50, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 224, 300)          13714500  \n",
      "_________________________________________________________________\n",
      "bidirectional_40 (Bidirectio (None, 100)               140400    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,855,001\n",
      "Trainable params: 13,855,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(vocab)+1, 300, weights = [embedding_matrix], input_length = 224, trainable = True, mask_zero = True)\n",
    "sequence_input = Input(shape=(224,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(50,dropout_W=0.2, dropout_U=0.2, return_sequences = True))(embedded_sequences)\n",
    "x = Bidirectional(LSTM(50,dropout_W=0.2, dropout_U=0.2, return_sequences = True))(embedded_sequences)\n",
    "x = Bidirectional(LSTM(50,dropout_W=0.2, dropout_U=0.2))(embedded_sequences)\n",
    "x = Dense(1,activation = 'sigmoid', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "model = Model(sequence_input, x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='AdaGrad',\n",
    "              metrics=['acc', keras_metrics.precision(), keras_metrics.recall()])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "7LpMsJoI4Z46",
    "outputId": "7fa4c46d-3b3b-485c-cc4b-7b6ac22e21b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 261975 samples, validate on 28341 samples\n",
      "Epoch 1/1000\n",
      "261975/261975 [==============================] - 464s 2ms/step - loss: 0.3625 - acc: 0.8559 - precision: 0.8580 - recall: 0.9631 - val_loss: 0.5764 - val_acc: 0.7458 - val_precision: 0.6811 - val_recall: 0.9445\n",
      "Epoch 2/1000\n",
      "261975/261975 [==============================] - 460s 2ms/step - loss: 0.2857 - acc: 0.8906 - precision: 0.8957 - recall: 0.9631 - val_loss: 0.5593 - val_acc: 0.7547 - val_precision: 0.6945 - val_recall: 0.9276\n",
      "Epoch 3/1000\n",
      "261975/261975 [==============================] - 459s 2ms/step - loss: 0.2643 - acc: 0.8997 - precision: 0.9053 - recall: 0.9644 - val_loss: 0.5714 - val_acc: 0.7560 - val_precision: 0.6981 - val_recall: 0.9201\n",
      "Epoch 4/1000\n",
      "261975/261975 [==============================] - 458s 2ms/step - loss: 0.2500 - acc: 0.9061 - precision: 0.9118 - recall: 0.9656 - val_loss: 0.5774 - val_acc: 0.7554 - val_precision: 0.6971 - val_recall: 0.9211\n",
      "Epoch 5/1000\n",
      "261975/261975 [==============================] - 458s 2ms/step - loss: 0.2403 - acc: 0.9104 - precision: 0.9157 - recall: 0.9669 - val_loss: 0.5717 - val_acc: 0.7590 - val_precision: 0.7090 - val_recall: 0.8956\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "history = model.fit(input_x_train, input_y_train, validation_data=(input_x_dev, input_y_dev), nb_epoch=1000, batch_size=1000, callbacks = [reduce_lr, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Q6atwgLaE7A",
    "outputId": "eebdc0fc-e66c-431d-ab22-767fc9149602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28341/28341 [==============================] - 429s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accr,precision,recall = model.evaluate(input_x_dev,input_y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1KoRiA_u1V2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "q6.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
